# -*- coding: utf-8 -*-
"""Data_sampler.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qC07B6v0JUrW-doZ7NEgKiJw6Ehu5FHh
"""

# Commented out IPython magic to ensure Python compatibility.
import os

try:
    from dotenv import load_dotenv
except ImportError:  # pragma: no cover - optional dependency
    load_dotenv = None

if load_dotenv is not None:
    load_dotenv()

NASA_Token = os.getenv("NASA_TOKEN")

if not NASA_Token:
    raise RuntimeError(
        "NASA_TOKEN environment variable is not set. "
        "Create a .env file (see .env.example) with your NASA Earthdata token before running downloads."
    )

# Optional: print current working directory for debugging
print(os.getcwd())


import os
import re
import time
import shutil
import requests
import numpy as np
import pandas as pd
import rasterio
from rasterio.windows import from_bounds
from rasterio.transform import from_bounds as rio_from_bounds
from rasterio.warp import reproject, Resampling
from rasterio.features import rasterize
import matplotlib.pyplot as plt
import geopandas as gpd
import h5py
import boto3
from botocore import UNSIGNED
from botocore.config import Config
import botocore
from PIL import Image
import gc
from collections import defaultdict
from datetime import datetime, timedelta
import os
import re
import numpy as np
import rasterio
from rasterio.windows import from_bounds
from rasterio.transform import from_bounds as rio_from_bounds
import h5py
import requests
import geopandas as gpd
import gc
import concurrent.futures
import os
import time
import concurrent.futures
from collections import defaultdict
import numpy as np
import rasterio
from rasterio.warp import reproject, Resampling
import boto3
from botocore import UNSIGNED
from botocore.config import Config
import botocore
from tqdm import tqdm

for folder in ["Raw_NL_Data", "For_Training"]:
    if os.path.isdir(folder):
        shutil.rmtree(folder)
        print(f"Removed {folder}")

# Paths
raster_path = 'Data/Global_2012/landscan-global-2012.tif'
shapefile_path = 'Data/World_Countries/World_Countries_Generalized.shp'

# Step 1: Load and downsample the raster
with rasterio.open(raster_path) as src:
    scale = 4  # Downsample factor, adjust as needed
    new_height = src.height // scale
    new_width = src.width // scale
    array = src.read(
        1,
        out_shape=(new_height, new_width),
        resampling=rasterio.enums.Resampling.bilinear
    )
    nodata = src.nodata
    transform = src.transform

if nodata is not None:
    array = np.where(array == nodata, np.nan, array)

# Step 2: Load shapefile and select Antarctica (FID==8)
gdf = gpd.read_file(shapefile_path)
antarctica = gdf[gdf['FID'] == 8]

# Step 3: Rasterize Antarctica polygon to downsampled raster shape
with rasterio.open(raster_path) as src:
    downsampled_transform = src.transform * src.transform.scale(
        (src.width / new_width),
        (src.height / new_height)
    )

mask = rasterize(
    [(geom, 1) for geom in antarctica.geometry],
    out_shape=(new_height, new_width),
    transform=downsampled_transform,
    fill=0,
    dtype='uint8'
)

# Generate latitude for each row
with rasterio.open(raster_path) as src:
    ys = np.linspace(
        src.bounds.top,
        src.bounds.bottom,
        new_height
    )
lat_mask = np.repeat(ys[:, np.newaxis], new_width, axis=1)
south_polar_mask = lat_mask < -60  # Boolean mask for everything below -60° latitude

# Combine Antarctica mask and south polar lat mask
combined_mask = (mask == 1) | south_polar_mask.astype(bool)

# Apply mask to array
array[combined_mask] = np.nan


# Step 4: Set Antarctica cells to NaN
array[mask == 1] = np.nan

# Step 5: Log-binning by integer bins [1,2), [2,3), ..., [10, inf)
log_array = np.log1p(array)
valid_mask = (~np.isnan(array)) & (array >= 0)
rows, cols = np.where(valid_mask)
logp = log_array[rows, cols]
pops = array[rows, cols]

# Bin setup: 1-2, 2-3, ..., 9-10, 10+
bin_edges = list(range(1, 11)) + [np.inf]
bin_labels = [f'{i}–{i+1}' for i in range(1, 10)] + ['10+']
# Color palette (10 distinct colors)
colors = ['red', 'orange', 'gold', 'green', 'teal', 'blue', 'purple', 'magenta', 'brown', 'black']
bins = np.digitize(logp, bin_edges) - 1

# Sample up to 5 from each bin, if possible
np.random.seed(2024)
sampled = []
for b, label in enumerate(bin_labels):
    idxs = np.where(bins == b)[0]
    if len(idxs) > 0:
        picks = np.random.choice(idxs, size=min(5, len(idxs)), replace=False)
        for pick in picks:
            sampled.append((rows[pick], cols[pick], logp[pick], pops[pick], label, colors[b]))

# Get geographic coordinates
with rasterio.open(raster_path) as src:
    scale_x = src.width / new_width
    scale_y = src.height / new_height
    coords = [src.xy(int(r*scale_y), int(c*scale_x)) for r, c, _, _, _, _ in sampled]

# Plot map and points (color-coded by bin)
plt.figure(figsize=(14, 7))
plt.imshow(log_array, cmap='viridis')
plt.title('LandScan 2012: 5 Samples per Integer Log Population Bin (Antarctica Masked)')
plt.axis('off')
plt.colorbar(label='log(population + 1)')
for (r, c, logv, pop, label, color) in sampled:
    plt.plot(c, r, 'o', markersize=8, color=color, label=label)
handles, labels_ = plt.gca().get_legend_handles_labels()
by_label = dict(zip(labels_, handles))
plt.legend(by_label.values(), by_label.keys())
plt.show()

# Print sample table
table = pd.DataFrame({
    'Bin': [x[4] for x in sampled],
    'Longitude': [c[0] for c in coords],
    'Latitude': [c[1] for c in coords],
    'Population': [int(round(x[3])) for x in sampled],
    'log(pop+1)': [float(x[2]) for x in sampled]
})
print(table.to_string(index=False))

# Helper: create bounding box (in degrees) centered at lon, lat, with size (in pixels)
def get_patch_bbox(lon, lat, patch_size_pix, tif_res_deg):
    half_deg = (patch_size_pix * tif_res_deg) / 2
    return [lon - half_deg, lat - half_deg, lon + half_deg, lat + half_deg]

# Helper: download Black Marble file for a bounding box and date
def search_nasa_cmr(collection_id, date_str, bbox):
    cmr_search_url = "https://cmr.earthdata.nasa.gov/search/granules.json"
    bbox_str = f"{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]}"
    params = {
        "collection_concept_id": collection_id,
        "temporal": f"{date_str}T00:00:00Z,{date_str}T23:59:59Z",
        "bounding_box": bbox_str,
        "page_size": 50
    }
    response = requests.get(cmr_search_url, params=params)
    h5_links = []
    if response.status_code == 200:
        granules = response.json().get('feed', {}).get('entry', [])
        for granule in granules:
            for link in granule.get('links', []):
                href = link.get('href', '')
                if href.startswith('https') and href.endswith('.h5'):
                    h5_links.append(href)
    return h5_links

# Helper: extract a patch from a GeoTIFF given lon, lat, patch size (pixels)
def extract_patch_from_geotiff(tif_path, lon, lat, patch_size_pix):
    with rasterio.open(tif_path) as src:
        tif_res_deg = src.transform[0]  # degrees per pixel (assuming square)
        bbox = get_patch_bbox(lon, lat, patch_size_pix, tif_res_deg)
        window = from_bounds(*bbox, src.transform)
        patch = src.read(1, window=window)
        patch_transform = src.window_transform(window)
        patch_meta = src.meta.copy()
        patch_meta.update({
            "height": patch.shape[0],
            "width": patch.shape[1],
            "transform": patch_transform
        })
    return patch, patch_meta

# Main: for each sample, download, convert, extract patch, save
def process_samples(sample_list, patch_size_pix, output_folder, collection_id, token, tile_shapefile_path, temp_folder="temp_dl"):
    os.makedirs(output_folder, exist_ok=True)
    os.makedirs(temp_folder, exist_ok=True)
    tile_shapefile = gpd.read_file(tile_shapefile_path)  # Load once

    for sample in sample_list:
        lon, lat, date_str = sample['Longitude'], sample['Latitude'], sample['date']

        # ⛔ Skip Antarctica (latitude south of -60 degrees)
        if lat < -60:
            print(f"Skipping Antarctica sample at ({lon:.3f}, {lat:.3f})")
            continue

        # 1. Search NASA for the file
        bbox = get_patch_bbox(lon, lat, patch_size_pix, tif_res_deg=0.004)  # VIIRS typical: 0.004 deg/pix

        # Prevent search below -60 degrees globally
        if bbox[1] < -60:
            bbox[1] = -60
        if bbox[3] < -60:
            print(f"Skipping search below -60°S for bbox {bbox}")
            continue

        urls = search_nasa_cmr(collection_id, date_str, bbox)
        if not urls:
            print(f"No Black Marble file found for {date_str} at ({lon:.3f}, {lat:.3f})")
            continue

        # 2. Download HDF5 file
        h5_url = urls[0]  # Pick the first match
        h5_path = os.path.join(temp_folder, os.path.basename(h5_url))
        if not os.path.exists(h5_path):
            r = requests.get(h5_url, headers={"Authorization": f"Bearer {token}"})
            with open(h5_path, "wb") as f:
                f.write(r.content)

        # 3. Convert HDF5 to GeoTIFF for the desired band
        tif_path = h5_path.replace(".h5", ".tif")
        with h5py.File(h5_path, "r") as f:
            ntl_path = "/HDFEOS/GRIDS/VIIRS_Grid_DNB_2d/Data Fields/Gap_Filled_DNB_BRDF-Corrected_NTL"
            if ntl_path not in f:
                print(f"⚠️ Skipping {h5_path}: dataset not found.")
                continue
            ntl_data = f[ntl_path][...]
            # Parse tile ID from filename
            tile_id_match = re.search(r'h\d{2}v\d{2}', h5_path)
            if not tile_id_match:
                print(f"⚠️ Could not determine tile_id from {h5_path}.")
                continue
            tile_id = tile_id_match.group()
            bounds_row = tile_shapefile[tile_shapefile['TileID'] == tile_id]
            if bounds_row.empty:
                print(f"⚠️ Tile ID {tile_id} not found in shapefile.")
                continue
            left, bottom, right, top = bounds_row.total_bounds
        with rasterio.open(tif_path, "w", driver="GTiff", height=ntl_data.shape[0], width=ntl_data.shape[1],
                           count=1, dtype=ntl_data.dtype, crs="EPSG:4326",
                           transform=rio_from_bounds(left, bottom, right, top, ntl_data.shape[1], ntl_data.shape[0])) as dst:
            dst.write(ntl_data, 1)

        # 4. Extract patch
        patch, patch_meta = extract_patch_from_geotiff(tif_path, lon, lat, patch_size_pix)

        # 5. Save patch as GeoTIFF
        out_path = os.path.join(output_folder, f"patch_{date_str}_{lon:.3f}_{lat:.3f}.tif")
        with rasterio.open(out_path, "w", **patch_meta) as dst:
            dst.write(patch, 1)

        # 6. Clean up temp files
        os.remove(h5_path)
        os.remove(tif_path)
        print(f"Saved patch: {out_path}")

    shutil.rmtree(temp_folder)

# Integer log-pop bins: [0,1), [1,2), ..., [10, inf)
bin_edges = list(range(0, 11)) + [np.inf]
bin_labels = [f'{i}–{i+1}' for i in range(0, 10)] + ['10+']
colors = [
    'dimgray',  # 0–1
    'red',      # 1–2
    'orange',   # 2–3
    'gold',     # 3–4
    'green',    # 4–5
    'teal',     # 5–6
    'blue',     # 6–7
    'purple',   # 7–8
    'magenta',  # 8–9
    'brown',    # 9–10
    'black'     # 10+
]

# Compute bins
bins = np.digitize(logp, bin_edges) - 1

# Sample up to 200 from each bin
np.random.seed(2024)
sampled = []
for b, label in enumerate(bin_labels):
    idxs = np.where(bins == b)[0]
    if len(idxs) > 0:
        picks = np.random.choice(idxs, size=min(200, len(idxs)), replace=False)
        for pick in picks:
            sampled.append((rows[pick], cols[pick], logp[pick], pops[pick], label, colors[b]))

# Get geographic coordinates for the samples
with rasterio.open(raster_path) as src:
    coords = [src.xy(int(r * scale_y), int(c * scale_x)) for r, c, *_ in sampled]

# Create plots folder if it doesn't exist
os.makedirs("plots", exist_ok=True)

# Plot map and points (color-coded by bin)
plt.figure(figsize=(14, 7))
plt.imshow(log_array, cmap='viridis')
plt.title('LandScan 2012: 200 Samples per Integer Log Population Bin')
plt.axis('off')
plt.colorbar(label='log(population + 1)')
for (r, c, logv, pop, label, color) in sampled:
    plt.plot(c, r, 'o', markersize=6, color=color, label=label)
handles, labels_ = plt.gca().get_legend_handles_labels()
by_label = dict(zip(labels_, handles))
plt.legend(by_label.values(), by_label.keys())

# Save plot
plt.savefig("plots/population_bins_sampled.pdf", bbox_inches='tight')
plt.close()

# Output table
table = pd.DataFrame({
    'Bin': [x[4] for x in sampled],
    'Longitude': [c[0] for c in coords],
    'Latitude': [c[1] for c in coords],
    'Population': [int(round(x[3])) for x in sampled],
    'log(pop+1)': [float(x[2]) for x in sampled]
})

# Uncomment to print table
# print(table.to_string(index=False))

def remove_with_retry(filepath, max_retries=5, delay=1):
    for attempt in range(max_retries):
        try:
            os.remove(filepath)
            return  # Success!
        except PermissionError:
            if attempt < max_retries - 1:
                print(f"PermissionError: File {filepath} is still in use. Retrying in {delay} sec...")
                time.sleep(delay)
            else:
                print(f"Failed to delete {filepath} after {max_retries} attempts.")
                raise  # Let the error propagate after max retries

def list_dmsp_dates(min_date=datetime(2012, 1, 20)):
    # Connect to NASA S3 bucket anonymously
    s3 = boto3.client("s3", config=Config(signature_version=UNSIGNED))
    bucket_name = "globalnightlight"
    prefix = "F"

    # List all objects with .vis.co.tif extension
    paginator = s3.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)
    all_dates = set()
    for page in page_iterator:
        for obj in page.get('Contents', []):
            file_key = obj['Key']
            if not file_key.endswith(".vis.co.tif"):
                continue
            fname = os.path.basename(file_key)
            if len(fname) < 11:
                continue
            date_str = fname[3:11]
            try:
                group_date = datetime.strptime(date_str, '%Y%m%d')
                if group_date >= min_date:
                    all_dates.add(date_str)
            except:
                continue
    # Return as sorted list of strings ('YYYYMMDD')
    return sorted(list(all_dates))

# Helper: create bounding box (in degrees) centered at lon, lat, with size (in pixels)
def get_patch_bbox(lon, lat, patch_size_pix, tif_res_deg):
    half_deg = (patch_size_pix * tif_res_deg) / 2
    return [lon - half_deg, lat - half_deg, lon + half_deg, lat + half_deg]

# Helper: always enforce desired patch size
def enforce_patch_size(patch, patch_size_pix):
    h, w = patch.shape
    patch = patch[:patch_size_pix, :patch_size_pix]
    pad_h = max(0, patch_size_pix - patch.shape[0])
    pad_w = max(0, patch_size_pix - patch.shape[1])
    if pad_h > 0 or pad_w > 0:
        patch = np.pad(
            patch, ((0, pad_h), (0, pad_w)),
            mode='constant', constant_values=0
        )
    return patch

# Helper: search NASA CMR for matching HDF5 file(s)
def search_nasa_cmr(collection_id, date_str, bbox):
    cmr_search_url = "https://cmr.earthdata.nasa.gov/search/granules.json"
    bbox_str = f"{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]}"
    params = {
        "collection_concept_id": collection_id,
        "temporal": f"{date_str}T00:00:00Z,{date_str}T23:59:59Z",
        "bounding_box": bbox_str,
        "page_size": 50
    }
    response = requests.get(cmr_search_url, params=params)
    h5_links = []
    if response.status_code == 200:
        granules = response.json().get('feed', {}).get('entry', [])
        for granule in granules:
            for link in granule.get('links', []):
                href = link.get('href', '')
                if href.startswith('https') and href.endswith('.h5'):
                    h5_links.append(href)
    return h5_links

# Helper: remove file with retry logic (for Windows file-locking)
def remove_with_retry(path, retries=5, delay=1):
    for _ in range(retries):
        try:
            if os.path.exists(path):
                os.remove(path)
            return
        except Exception as e:
            import time
            time.sleep(delay)

# Helper: extract a patch from a GeoTIFF given lon, lat, patch size (pixels)
def extract_patch_from_geotiff(tif_path, lon, lat, patch_size_pix):
    with rasterio.open(tif_path) as src:
        tif_res_deg = src.transform[0]  # degrees per pixel (assuming square)
        bbox = get_patch_bbox(lon, lat, patch_size_pix, tif_res_deg)
        window = from_bounds(*bbox, src.transform)
        patch = src.read(1, window=window)
        patch = enforce_patch_size(patch, patch_size_pix)
        patch_transform = src.window_transform(window)
        patch_meta = src.meta.copy()
        patch_meta.update({
            "height": patch_size_pix,
            "width": patch_size_pix,
            "transform": patch_transform
        })
    return patch, patch_meta

# Per-sample worker function
def process_single_sample(sample, patch_size_pix, collection_id, token, tile_shapefile, output_folder, temp_folder):
    lon, lat, date_str = sample['Longitude'], sample['Latitude'], sample['date']
    bbox = get_patch_bbox(lon, lat, patch_size_pix, tif_res_deg=0.004)  # VIIRS typical: 0.004 deg/pix
    urls = search_nasa_cmr(collection_id, date_str, bbox)
    if not urls:
        return f"No Black Marble file found for {date_str} at ({lon:.3f}, {lat:.3f})"
    h5_url = urls[0]  # Pick the first match
    h5_path = os.path.join(temp_folder, os.path.basename(h5_url))
    if not os.path.exists(h5_path):
        r = requests.get(h5_url, headers={"Authorization": f"Bearer {token}"})
        with open(h5_path, "wb") as f:
            f.write(r.content)
    tif_path = h5_path.replace(".h5", ".tif")
    with h5py.File(h5_path, "r") as f:
        ntl_path = "/HDFEOS/GRIDS/VIIRS_Grid_DNB_2d/Data Fields/Gap_Filled_DNB_BRDF-Corrected_NTL"
        if ntl_path not in f:
            remove_with_retry(h5_path)
            return f"⚠️ Skipping {h5_path}: dataset not found."
        ntl_data = f[ntl_path][...]
        tile_id_match = re.search(r'h\d{2}v\d{2}', h5_path)
        if not tile_id_match:
            remove_with_retry(h5_path)
            return f"⚠️ Could not determine tile_id from {h5_path}."
        tile_id = tile_id_match.group()
        bounds_row = tile_shapefile[tile_shapefile['TileID'] == tile_id]
        if bounds_row.empty:
            remove_with_retry(h5_path)
            return f"⚠️ Tile ID {tile_id} not found in shapefile."
        left, bottom, right, top = bounds_row.total_bounds
    with rasterio.open(tif_path, "w", driver="GTiff", height=ntl_data.shape[0], width=ntl_data.shape[1],
                       count=1, dtype=ntl_data.dtype, crs="EPSG:4326",
                       transform=rio_from_bounds(left, bottom, right, top, ntl_data.shape[1], ntl_data.shape[0])) as dst:
        dst.write(ntl_data, 1)
    patch, patch_meta = extract_patch_from_geotiff(tif_path, lon, lat, patch_size_pix)
    if patch.shape[0] < patch_size_pix or patch.shape[1] < patch_size_pix:
        gc.collect()
        remove_with_retry(h5_path)
        remove_with_retry(tif_path)
        return f"❌ Patch at ({lon:.3f}, {lat:.3f}) on {date_str} is too small ({patch.shape[0]}x{patch.shape[1]}). Skipping."
    out_path = os.path.join(
        output_folder, f"BM_patch_{date_str}_{lon:.3f}_{lat:.3f}.tif"
    )
    with rasterio.open(out_path, "w", **patch_meta) as dst:
        dst.write(patch, 1)
    gc.collect()
    remove_with_retry(h5_path)
    remove_with_retry(tif_path)
    return f"Saved patch: {out_path}"

# Main function: parallel processing
from tqdm import tqdm

def process_samples_parallel(
    sample_list, patch_size_pix, collection_id, token, tile_shapefile_path,
    output_folder="raw NL data/BM data", temp_folder="temp_dl", max_workers=4
):
    os.makedirs(output_folder, exist_ok=True)
    os.makedirs(temp_folder, exist_ok=True)
    tile_shapefile = gpd.read_file(tile_shapefile_path)
    args = [
        (sample, patch_size_pix, collection_id, token, tile_shapefile, output_folder, temp_folder)
        for sample in sample_list
    ]
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_single_sample, *arg) for arg in args]
        for f in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc="Patches processed", unit="patch"):
            try:
                print(f.result())
            except Exception as e:
                print(f"Error in thread: {e}")

# Step 1: Get your list of DMSP dates
all_dmsp_dates = list_dmsp_dates()  # returns list like ['20120121', '20120122', ...]

# Step 2: Sample as many dates as you have table rows
import random
random.seed(13492)
sampled_dates = random.choices(all_dmsp_dates, k=len(table))

# Step 3: Assign to your DataFrame's 'date' column, converting 'YYYYMMDD' to 'YYYY-MM-DD'
table['date'] = [f'{d[:4]}-{d[4:6]}-{d[6:8]}' for d in sampled_dates]

# Step 4: Build your sample list of dicts
sample_list = table[['Longitude', 'Latitude', 'date']].to_dict(orient='records')

# Step 5: Now you can safely call the parallel function!
process_samples_parallel(
    sample_list=sample_list,
    patch_size_pix=256,
    collection_id="C3365931269-LAADS",
    token=NASA_Token,
    tile_shapefile_path="Data/Black_Marble_IDs/Black_Marble_World_tiles.shp",
    output_folder="Raw_NL_Data/BM data",
    temp_folder="temp_dl",
    max_workers=4
)

import os
import time
import concurrent.futures
from collections import defaultdict
import numpy as np
import rasterio
from rasterio.warp import reproject, Resampling
import boto3
from botocore import UNSIGNED
from botocore.config import Config
import botocore
from tqdm import tqdm

def wait_for_file_release(path, timeout=10):
    start = time.time()
    while True:
        try:
            with open(path, 'rb'):
                return True
        except PermissionError:
            if time.time() - start > timeout:
                raise
            time.sleep(0.5)

def safe_download(s3, bucket, key, outpath, max_retries=5):
    for attempt in range(max_retries):
        try:
            s3.download_file(bucket, key, outpath)
            wait_for_file_release(outpath)
            return True
        except botocore.exceptions.EndpointConnectionError as e:
            print(f"EndpointConnectionError on {key} (attempt {attempt+1}/{max_retries}): {e}")
        except botocore.exceptions.ClientError as e:
            print(f"ClientError on {key} (attempt {attempt+1}/{max_retries}): {e}")
        except Exception as e:
            print(f"Other error on {key} (attempt {attempt+1}/{max_retries}): {e}")
        time.sleep(2)
    print(f"Failed to download after {max_retries} attempts: {key}")
    return False

def group_by_f_number(file_keys):
    groups = defaultdict(list)
    for vis_key, _ in file_keys:
        base = os.path.basename(vis_key)
        f_number = base.split('_')[0] if '_' in base else base[:3]
        groups[f_number].append((vis_key, None))
    return groups

def reproject_to_bm_grid(src_path, bm_profile):
    with rasterio.open(src_path) as src:
        src_data = src.read(1).astype(np.float32)
        src_data[src_data == 255] = np.nan  # Mask 255 as NaN immediately!
        dst = np.empty((bm_profile['height'], bm_profile['width']), dtype=np.float32)
        reproject(
            source=src_data,
            destination=dst,
            src_transform=src.transform,
            src_crs=src.crs,
            dst_transform=bm_profile['transform'],
            dst_crs=bm_profile['crs'],
            resampling=Resampling.bilinear,
            src_nodata=np.nan,
            dst_nodata=np.nan
        )
    return dst

def process_bm_patch_for_best_fnumber(bm_patch_path, file_keys, s3, bucket_name, out_dir, dmsp_out_dir):
    bm_files_saved = []
    bm_patch_name = os.path.basename(bm_patch_path)
    with rasterio.open(bm_patch_path) as bm_src:
        bm_profile = bm_src.profile.copy()
        bm_shape = (bm_src.height, bm_src.width)

    groups = group_by_f_number(file_keys)
    for f_number, scene_keys in groups.items():
        best_valid_pixels = 0
        best_vis_file = None
        best_vis_patch = None
        for vis_key, _ in scene_keys:
            vis_file = os.path.join(out_dir, os.path.basename(vis_key))
            if not os.path.exists(vis_file):
                print(f"Downloading {vis_key} ...")
                safe_download(s3, bucket_name, vis_key, vis_file)
            try:
                vis_patch = reproject_to_bm_grid(vis_file, bm_profile)
                valid_pixels = np.sum(~np.isnan(vis_patch))
                median_val = np.nanmedian(vis_patch) if valid_pixels > 0 else 0
                if valid_pixels > best_valid_pixels and median_val > 1:
                    best_valid_pixels = valid_pixels
                    best_vis_file = vis_file
                    best_vis_patch = vis_patch
            except Exception as e:
                print(f"Error processing {vis_file}: {e}")
                continue
        if best_vis_file and best_valid_pixels > 0:
            valid_fraction = best_valid_pixels / (bm_shape[0] * bm_shape[1])
            if valid_fraction < 0.10:
                print(f"Skipping {f_number} for {bm_patch_name}: only {valid_fraction:.2%} valid pixels")
                continue
            print(f"Best patch for {f_number} has {best_valid_pixels} valid pixels ({valid_fraction:.2%})")
            out_fname = f"{f_number}_{os.path.basename(best_vis_file).split('.')[0]}_match_{bm_patch_name.replace('.tif','')}.tif"
            out_path = os.path.join(dmsp_out_dir, out_fname)
            out_profile = bm_profile.copy()
            out_profile.update({
                "dtype": "float32",
                "count": 1,
                "nodata": np.nan,
            })
            os.makedirs(os.path.dirname(out_path), exist_ok=True)
            try:
                with rasterio.open(out_path, "w", **out_profile) as dst:
                    dst.write(best_vis_patch.astype(np.float32), 1)
                print(f"✅ Saved {out_path}")
                bm_files_saved.append(out_path)
            except Exception as e:
                print(f"Could not extract/save patch from {best_vis_file}: {e}")
        else:
            print(f"No good patch found for {f_number} in {bm_patch_name}")
    return bm_files_saved

def parallel_process_bm_patch(args):
    (bm_patch_path, s3, bucket_name, temp_dir, dmsp_out_dir) = args
    bm_date = os.path.basename(bm_patch_path).split('_')[2]
    dmsp_date_str = bm_date.replace("-", "")
    print(f"Processing BM patch: {os.path.basename(bm_patch_path)}")
    file_keys = []
    satellites = [f'F{n}' for n in range(10, 19)]
    for sat in satellites:
        prefix = f"{sat}{dmsp_date_str[:4]}/"
        paginator = s3.get_paginator('list_objects_v2')
        for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):
            for obj in page.get('Contents', []):
                key = obj['Key']
                if dmsp_date_str in key and key.endswith('.vis.co.tif'):
                    file_keys.append((key, None))
    saved = process_bm_patch_for_best_fnumber(bm_patch_path, file_keys, s3, bucket_name, temp_dir, dmsp_out_dir)
    for f in os.listdir(temp_dir):
        try:
            os.remove(os.path.join(temp_dir, f))
        except Exception as e:
            print(f"Error deleting {f}: {e}")
    return saved

# --- MAIN PARALLEL LOOP ---
bm_patch_dir = "Raw_NL_Data/BM data"
dmsp_out_dir = "Raw_NL_Data/DMSP data"
temp_dir = "DMSP_Raw_Temp"
os.makedirs(temp_dir, exist_ok=True)

bm_files = [f for f in os.listdir(bm_patch_dir) if f.endswith('.tif')]
bm_patch_paths = [os.path.join(bm_patch_dir, f) for f in bm_files]
s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))
bucket_name = 'globalnightlight'

max_workers = 4  # Adjust for your environment

with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
    args_list = [(bm_patch_path, s3, bucket_name, temp_dir, dmsp_out_dir) for bm_patch_path in bm_patch_paths]
    futures = [executor.submit(parallel_process_bm_patch, arg) for arg in args_list]
    for f in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc="Patches processed", unit="patch"):
        try:
            _ = f.result()  # or print(f.result()) to see output
        except Exception as e:
            print(f"Error: {e}")

bm_dir = r"Raw_NL_Data/BM data"
dmsp_dir = r"Raw_NL_Data/DMSP data"

# Preload all DMSP filenames for fast lookup
dmsp_files = os.listdir(dmsp_dir)

# Build a set of substrings for quick membership check (for performance)
dmsp_filename_str = "\n".join(dmsp_files)  # single string, so 'in' is fast

# Regex to parse BM patch info
bm_pattern = re.compile(r"BM_patch_(\d{4}-\d{2}-\d{2})_([-+]?\d*\.\d+|\d+)_([-+]?\d*\.\d+|\d+)\.tif$")

removed_count = 0
for bm_file in os.listdir(bm_dir):
    match = bm_pattern.match(bm_file)
    if not match:
        print(f"Skipping unrecognized file: {bm_file}")
        continue
    date, lat, lon = match.groups()
    key = f"BM_patch_{date}_{lat}_{lon}"

    # Look for any DMSP file containing this BM key
    if key not in dmsp_filename_str:
        # No match: remove this BM patch
        bm_path = os.path.join(bm_dir, bm_file)
        os.remove(bm_path)
        print(f"Removed: {bm_file}")
        removed_count += 1

print(f"Done. Removed {removed_count} BM patches with no DMSP match.")

import os
import re
import rasterio
import numpy as np
from PIL import Image
import shutil
from tqdm import tqdm  # <-- Progress bar


# Clean up
for folder in ["For_Training","For_Training"]:
    if os.path.isdir(folder):
        shutil.rmtree(folder)
        print(f"Removed {folder}")

bm_dir = r"Raw_NL_Data/BM data"
dmsp_dir = r"Raw_NL_Data/DMSP data"
output_root = r"For_Training"
bm_out = os.path.join(output_root, "BM")
dmsp_out = os.path.join(output_root, "DMSP")
os.makedirs(bm_out, exist_ok=True)
os.makedirs(dmsp_out, exist_ok=True)



patch_pattern = re.compile(r"(BM_patch_\d{4}-\d{2}-\d{2}_[-+]?\d*\.\d+_[-+]?\d*\.\d+)")

# Index BM patches for quick access
bm_files = os.listdir(bm_dir)
bm_key_to_file = {}
for bm_file in bm_files:
    m = patch_pattern.search(bm_file)
    if m:
        bm_key_to_file[m.group(1)] = bm_file

# Collect all (dmsp_file, bm_file) pairs
dmsp_files = os.listdir(dmsp_dir)
pair_list = []
for dmsp_file in dmsp_files:
    m = patch_pattern.search(dmsp_file)
    if not m:
        continue
    patch_key = m.group(1)
    bm_file = bm_key_to_file.get(patch_key)
    if bm_file:
        pair_list.append((dmsp_file, bm_file, patch_key))

pair_list.sort(key=lambda x: (x[2], x[0]))  # Sort for reproducibility

def augmentations(img):
    angles = [0, 90, 180, 270]
    mirror_opts = [False, True]
    vflip_opts = [False, True]
    for angle in angles:
        for mirror in mirror_opts:
            for vflip in vflip_opts:
                aug = img.rotate(angle, expand=True)
                if mirror:
                    aug = aug.transpose(Image.FLIP_LEFT_RIGHT)
                if vflip:
                    aug = aug.transpose(Image.FLIP_TOP_BOTTOM)
                yield aug

def tif_to_png_16bit(tif_path, scale_factor=4):
    with rasterio.open(tif_path) as src:
        array = src.read(1)
        arr = np.nan_to_num(array)
        if np.issubdtype(arr.dtype, np.floating):
            arr = arr - arr.min()
            if arr.max() > 0:
                arr = (arr / arr.max()) * 65535
            arr = arr.astype(np.uint16)
        elif arr.dtype != np.uint16:
            arr = arr.astype(np.uint16)
        img = Image.fromarray(arr, mode='I;16')
        img_resizable = img.convert('I')
        new_size = (img.width // scale_factor, img.height // scale_factor)
        try:
            img_resized = img_resizable.resize(new_size, resample=Image.Resampling.LANCZOS)
        except AttributeError:
            img_resized = img_resizable.resize(new_size, resample=Image.LANCZOS)
        img_final = img_resized.convert('I;16')
        return img_final

# Count total number of patches for progress bar
num_augs = 4 * 2 * 2  # 4 angles, 2 mirror, 2 vflip = 16
total = len(pair_list) * num_augs

patch_counter = 1
with tqdm(total=total, desc="Saving paired patches", unit="patch") as pbar:
    for dmsp_file, bm_file, patch_key in pair_list:
        dmsp_img = tif_to_png_16bit(os.path.join(dmsp_dir, dmsp_file), scale_factor=4)
        bm_img   = tif_to_png_16bit(os.path.join(bm_dir, bm_file), scale_factor=1)

        for dmsp_aug, bm_aug in zip(augmentations(dmsp_img), augmentations(bm_img)):
            patch_name = f"patch{patch_counter:04d}.png"
            dmsp_aug.save(os.path.join(dmsp_out, patch_name))
            bm_aug.save(os.path.join(bm_out, patch_name))
            patch_counter += 1
            pbar.update(1)

print(f"\nDone! Saved {patch_counter-1} perfectly matched DMSP/BM PNGs in {dmsp_out} and {bm_out}")

import shutil
import os

output_root = "For_Training_RGB"

# Remove the folder and all its contents if it exists
if os.path.isdir(output_root):
    shutil.rmtree(output_root)
    print(f"Removed {output_root}")
else:
    print(f"{output_root} does not exist.")

import numpy as np
from PIL import Image
import os
from tqdm import tqdm

# Input/output folders
input_root = "For_Training"
output_root = "For_Training_RGB"
bm_in  = os.path.join(input_root, "BM")
dmsp_in = os.path.join(input_root, "DMSP")
bm_out = os.path.join(output_root, "BM")
dmsp_out = os.path.join(output_root, "DMSP")
os.makedirs(bm_out, exist_ok=True)
os.makedirs(dmsp_out, exist_ok=True)

def convert_16bit_to_16bit(input_folder, output_folder, noise_lambda=2):
    files = [f for f in os.listdir(input_folder) if f.lower().endswith('.png')]
    for fname in tqdm(files, desc=f"Converting {os.path.basename(input_folder)}"):
        src_path = os.path.join(input_folder, fname)
        dst_path = os.path.join(output_folder, fname)
        img = Image.open(src_path)
        arr = np.array(img)
        if arr.dtype != np.uint16:
            arr = arr.astype(np.uint16)
        if arr.max() == 0:
            # Replace with Poisson noise in 16-bit range
            arr = np.random.poisson(lam=noise_lambda, size=arr.shape).astype(np.uint16)
        # Save as single-channel 16-bit PNG
        out_img = Image.fromarray(arr, mode='I;16')
        out_img.save(dst_path)

# Usage
convert_16bit_to_16bit(bm_in, bm_out, noise_lambda=2)
convert_16bit_to_16bit(dmsp_in, dmsp_out, noise_lambda=2)

for folder in ["DMSP_Raw_Temp", "temp_dl"]:
    if os.path.isdir(folder):
        shutil.rmtree(folder)
        print(f"Removed {folder}")

!git add .
!git commit -m "Auto push from Jupyter"
!git push origin main